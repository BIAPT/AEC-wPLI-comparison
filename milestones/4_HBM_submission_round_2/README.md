# Human Brain Mapping Submission_2
This version is very similar to the previous milestone. We got some new team members responsible for the code of this and wanted to keep one version as it was before. The previous version is a more complex version containing many parameters and possibilities for further exploration. 

For everyone who is interested in a wide exploration of for example wighted and binarized graphs, please refer to milestone 3. 

The recent milestone is a more limited version which only contains the parameters and analysis done for the submission of the study. The purpose of this milestone is to have one version of code, which has been used in exactly this format to submit the study. 

## Preparation 
- Make sure to have a compute canada account on the Beluga cluster.

- Follow the section [Using the MATLAB Parallel Server](https://docs.computecanada.ca/wiki/MATLAB) on compute canada documentation

- Create a project folder inside your account. Inside there, create folders named "data" and "results"

- clone this code, as well as Neuroalgo toolbox in your Compute Canada account in this folder

- log into your account and adapt the e-mail address for your notifications in all .sl files. This will send you a notification when your job begins and ends. 

- Get the cleaned source localized EEG data of all 9 participant (these are .mat files generated by BrainStorm)

- Transfer it inside your HPC folder "data" using [Globus](https://globus.computecanada.ca/)

  ## Notes:

  - step 2,3 and 4 are written in python. All jobs can be executed from the main folder by typing  `sbatch jobname.sl `  the job will be submitted to the cluster. Please make sure to adapt all .sl files to contain your e-mail address. Once the job is running it will output a .out file. This file contains the job output. When a job fails have a look into this jobID.out file to identify the error.   
  - 

## Step 0: Generate the AEC and wPLI Graphs
- navigate to the folder `step_0_generate_graphs/`.
- Move to the `generate_aec_graphs` subfolder. Open the job.sl and modify the parameters to match the resource you want to use and your account on Compute Canada.
- Open the `generate_aec.m` file and modify the parameter relating to your cluster setup and path. There are some path that needs to be modified in order for the input/output to make sense in your section of the cluster. 
- Once ready you can run the following commands: `sbatch job.sl`, this will send the job to your cluster
- Repeat the same procedure for `generate_wpli_graphs`. You do not have to wait for the AEC graph to be done before you start running the wPLI computation. Both code use the same EEG source localized data as input, but they do not step on each other while calculating the graphs.

**Word of Caution:** The speed up that you can gain from the parallelization really depends on what is the availability of the cluster. If you try to use up to 960 cores you might wait a long time before it becomes available (I've waited 8+h and still wasn't scheduled). However, if you use only 40 cores on 1 node you will most likely get scheduled right away. There is a balance to strike and it is still not obvious what is the best course of action. Sometime you get lucky sometime you don't.

--> After this step you should have all graphs needed for further calculation in the results/graphs folder

## Step 1: Generate the Feature Dataframe
- Navigate into the folder `step_1_generate_features`. Once there open the job.sl and modify the parameters to match the resource you want to use and your account on compute Canada. 
- Open the file `generate_features.m` and modify the input/output folder to match what you have in step 1. You can also change the parameter of the analysis if needs be.
- Once ready you can run the following commands: `sbatch job.sl` assuming the cluster variable is already assigned as show in step 1.

## Step 2: Run the model selection with LOSO cross validation
- Open the commons.py file and make sure that the input and output are correct for your HPC setup and define which epoch, graph and feature category you want to select

- crate a folder called  "models" in the "results" folder

- Then adapt and run `step_2a_run_all_models.sl` 

  --> this should fill the "models" folder with a summary of all possible models

- after the job has finished, adapt and run `step_2b_visualize_all_models.sl`

  --> this should output a .pdf and .txt with the summary of all models. The pdf contains the visual summary of each model with the averaged accuracy and f1 score in the title. The .txt outputs all accuracies and the model with the highest accuracy and F1. 

  --> Download the pdf and txt and have a look at them

- open the commons.py and select the best_model to be the model with the highest accuracy. 

## Step 3: Run the Final model and visualize the summary

- Open the commons.py file and make sure that the parameter "best_model" is the model you have selected as your best performing model. (based on the previous step) 

- crate a folder called  "final_models" in the "results" folder

- Then adapt and run `step_3a_run_final_model.sl` 

  --> this should fill the "final_models" folder with a summary of the final model's performance in all conditions

- after the job has finished, adapt and run `step_3b_visualize_models.sl`

  --> this should output several .csv files with the accuracy and F1 score for the model in all conditions  

  --> Download these csv files. These are the FINAL ACCURACIES

## Step 4: Run the Final model bootstrap and permutation

- Open the commons.py file and make sure that the parameter "best_model" is the model you have selected as your best performing model. (based on the previous step) 

- crate a folder called  "bootstrap" and "permutation" in the "results" folder

- Inside the step 4 folder, adapt  `job_bootstarp.sl and job_permutation.sl`  to contain your e-mail address

- navigate back to the subfolder  and open "generate_jobs.bash". This bash code contains all conditions you'll run the analysis on. It works like a for loop within a job but submits one individual job for each iteration. This has the advantage of having lots of small jobs instead of one giant job. This reduces the waiting time significantly.  

- Adapt the conditions you want to run and run the following command ` bash generate_jobs.bash step_4_characterize_classification/job_permutation.sl` 

  --> this should fill the "permutation" folder with excel files for all conditions 

- Adapt the conditions you want to run and run the following command ` bash generate_jobs.bash step_4_characterize_classification/job_bootstrap.sl` 

  --> this should fill the "bootstrap" folder with excel files for all conditions 

- You can now download these folders

## Step 5: Visualize the features

- Open the commons.py file and make sure that the parameter "best_model" is the model you have selected as your best performing model. (based on the previous step) 

- adapt and submit the job ` job_step_5_extract_weights.sl` 

  --> it will output you two .csv files  called "feature_weights" + stepsize

- download these and run the figure generation on your own computer to monitor the process: follow the next steps to do so

- copy the feature_weight csv into the step_5_generate_figures/plotting folder. 

- create an empty folder called "figures" (this is where all figures will be saved") 

- Open and adapt ` plot_brain_weights.m`  

  --> this will output many .fig files in the "figures" folder 

  (the parameter features corresponds to the feature which is selected when both graphs are inside the classifier)

  

- 